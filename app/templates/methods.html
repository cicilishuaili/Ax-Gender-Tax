{% extends "basic-layout.html" %}

    {% block maincontent %}
        <h2>Methods</h2>
        <h3>Data Source</h3>
          <p>There is no online market more ubiquitous than Amazon. The product descriptions can be available via their <a href="https://docs.aws.amazon.com/AWSECommerceService/latest/DG/Welcome.html">Amazon Product Advertisement API</a>, requiring an associates account that unfortunately became restricted over the course of the project. Although it is a bit restrictive in the amount of information that is accessible (e.g. they only allow a search to fetch 10 pages of results), it served as initial exploration that revealed useful insights.</p>

          <p>As a result of the restricted access to Amazon's API, the alternative that became the main source of data was the <a href="https://developer.walmartlabs.com/">Walmart Open API.</a> Although it is highly error prone, heterogeneous, and less reliable overall than its Amazon counterpart, it conveniently provide a "gender" category for most of the items and has no restriction on the results returned.</p>

        <h3>Analysis</h3>
          <p>At the heart of this analysis are <a href="/methods#NB">Naive Bayes</a> and <a href="/methods#Regression">regression</a>. This demonstration currently only pertains to deodorants, but these techniques are intended to be generalizable to all product categories.</p>

        <h4>Pre-processing</h4>
          <p>Text had to be processed using standard NLP techniques from Python's Natural Language Toolkit (nltk). After going through a standard tokenizer, punctuations and stopwords were filtered out explicitly along with the words 'men'/'man' and 'women'/'woman'. A decision was made against lemmatization and stemming because of the existence of subtle and interesting nuisances, e.g. in deodorants, 'moisturizer' singular for men vs. 'moisturizers' plural for women and 'lady' for women, 'ladies' for men. Only unigrams were considered for simplicity. This resulted in a simple bag of words model for the Naive Bayes classifier.</p>

          <p>For the price regressor, there many different ways to potentially vectorize the words. Ultimately, to prevent the complete linear response to count in presence of a word, tf-idf vectorizer was used. There is some implicit filtering out of stopwords with min and max df as a result</p>

        <div id="NB">
        <h4>Naive Bayes</h4>
          <p>The classifier used for gender is based on the Naive Bayes algorithm, with features being the bag of words from the descriptions. In order to find the probability for a label, this algorithm first uses the Bayes rule to express P(label|features) in terms of P(label) and P(features|label). The algorithm then makes the ‘naive’ assumption that all features are independent, given the label.</p>

            <p>Rather than computing P(features) explicitly, the algorithm just calculates the numerator for each label, and normalizes them so they sum to one. For this tool, if the probability
            </p>
        <!--<div id="masculine">
            <h3>Masculine-skewed words</h3>
            <ul>
                {% for word in masculine_coded_words %}
                <li>{{ word }}</li>
                {% endfor %}
            </ul>
        </div>
        <div id="feminine">
            <h3>Feminine-skewed words</h3>
            <ul>
                {% for word in feminine_coded_words %}
                <li>{{ word }}</li>
                {% endfor %}
            </ul>
        </div>-->
        <p>The Naive Bayes algorithm used is very simple and interpretable, so the products with gender that can be somewhat accurately predicted by this method have distinguishing words that appear mostly in one category. The set up is also very straightforward, with a 80/20 split for training/testing data sets. With that said, the test accuracy is surprisingly high, in the high 70s to mid 80s for things like razors, perfume, shirt, pants, and wallets. Keep in mind that a human may not be able to predict whether a product description is one for women's or men's product with 100% accuracy, especially with indicating words removed.
        </p>

        <p>For the purpose of this function, the informativeness of a word(feature) is equal to the highest value of P(feature|label), for any label, divided by the lowest value of P(feature|label), for any label.</p>
        <p><img src="/static/documents/razors1.png" width="600" height="600"> Training accuracy: 93.8811188811
 Test accuracy: 84.6153846154</p>
        <p><img src="/static/documents/razors2.png" width="600" height="600"> Training accuracy: 93.5897435897
 Test accuracy: 87.7551020408</p>

        <p>Removing the categories of "razors with soap bars" for women and "safety razors" and "straight razors" for men definitely has a noticeable difference. One of the more obvious being the lack of "straight" as an informative word for men. It also removed "moisturizing" for women. Interestingly though, it also removed "Venus", which I can only presume made a lot of those razors with soap bars, such as the "embrace".</p>

        <p>Seems that "beard" and "professional" are more associated with safety and straight razors as they were no longer there after the filter.</p>

        <p>Some more reflections of practical features are "legs" for women. The name "intuition" as a razor for women stuck, and so has "butter" which implies some moisturizing still.

        <p>Qualitatively, "sharp", "edge", "angle", "precision" as informative words for men is not too surprising given the general connotations. As opposed to the "silky" and "curves" that appeared for women before. Who also appear to value "convenience" more over a "superior" experience.</p>
        <p><img src="/static/documents/perfume1.png" width="600" height="600"> Training accuracy: 88.5245901639
 Test accuracy: 77.3076923077</p>
        <p><img src="/static/documents/perfume2.png" width="600" height="600"> Training accuracy: 88.7605042017
 Test accuracy: 78.9915966387</p>
        <p>Despite filtering out the "essential oils" category for women, "essential" still makes appearance as the most indicative word for women.</p>

        <p>"Womenly" scents include aloe, orchid, freesia. "Manly" scents include rosewood, sage, tobacco, cardamom. Unsurprisingly, it is "flowery" vs. "spices" (and other so-called rugged things like ashes and timber).</p>

        <p>The main points that catch attention are the adjectives "delicate" and "pure" for women, compared with "manly", "masculine", "man", and "homme" for men. It seems important to emphasize the masculinity of the product for men. Although given the lack of the word feminine for women, it doesn't appear that it is something perfumes for women want to highlight either. Nonetheless, the liberal application of such seemingly subjective adjectives is fascinating.</p>
        <p><img src="/static/documents/pants.png" width="600" height="600"> Training accuracy: 90.652173913
 Test accuracy: 88.2608695652</p>
        <p><img src="/static/documents/jeans.png" width="600" height="600"> Training accuracy: 92.2330097087
 Test accuracy: 81.8181818182</p>
        <p><img src="/static/documents/shirts.png" width="600" height="600"> Training accuracy: 92.9505813953
 Test accuracy: 85.1744186047</p>
        <p><img src="/static/documents/socks.png" width="600" height="600"> Training accuracy: 91.0792951542
 Test accuracy: 78.3185840708</p>
        <p><img src="/static/documents/wallets.png" width="600" height="600"> Training accuracy: 98.064516129
 Test accuracy: 78.9473684211</p>

        <p>Things like shoes and watches, on the hand seem to do less well. Their accuracies on the test cases falls below 70s, although it is debatable if humans could do much better. And for glasses, it seems to be so bad as to draw with random assignment at 50% accuracy.</p>

        <p>It could have more to do with the quality of descriptions for these descriptions. They may contain a large amount of "nondescript" desriptions that tend to be brief and generic, or messy and filled with technical measurements, copy and pasted over and over. The subcategories may also differ too much to create a coherent theme, as it may be the case for glasses, which included cases and chains.</p>

</div>

<div id="Regression">
  <h4>Regression</h4>

  <p>The prices were not easily characterized. Walmart often has absurd prices for items that were currently not in stock or substantially inflated to reflect availability. Correction was attempted by considering only items with available inventory status. More problematic was the fact that there were no good way to normalize the price by amount, as quantity information is extremely non-standard and messy. Looking at the broad distribution, we can see a long tail at the higher price end due to the "bulk" packaging multiple items. Due to this reason, outliers in price can remain and no other accesbile checks could be done to systematically detect and discard them.<p/>

  <p><img src="/static/documents/pricew.png"><img src="/static/documents/pricem.png"></p>

  <p>We can still make valid comparisons overall, assuming that all others are held equal between the two gender markets, e.g., if we can buy a box of deodorants for men, we can do the same for women, pricing surges and dips are similar overall, etc. Due to the skewed distribution, the median was used as the average for comparisons of prices. For deodorants, this was $9.9 for women, and $9.17 for male, translating to about 8% difference, right on par with the 7% that was found in the <a href="/static/documents/Study-of-Gender-Pricing-in-NYC.pdf">study of gender pricing in NYC</a>.<p/>

  <p>Before the Amazon API became restricted, I was able to perform some initial analysis of razor prices from the site. Interesting, I observed, though cannot show in graphics, that over all razors, those for men tend to be more expensive. Of course in the context of a broader category, the comparison is less valid. It is suspected that power razors for men was able to drive up the overall cost. Despite not being able to investigate further, this tool is decidedly for both men and women, whoever is paying more for a given individual product.<p/>


  <p>Several models were tested for regression on the price, which turns out to be non-trivial due again to the fact that prices often depend heavily on quantity information which may or may not be apparent in the description. These ranged from linear models to multi-layer perceptrons, none of which performed too well on out-of-sample metrics. Powerful techniques tended to overfit easily despite cross validation. Nonetheless, the main objective is to provide a potential price differential due to presence of potentially gendered words, so overall accuracy was traded off for ease of interpretability and generalizable association.<p/>

    <p>The crux of this analysis remains the fact only care about those words that are both gendered (see definition of high impact in previous section), and has an impact on price. This is key because some of the gendered words may imply a certain design feature that the user desires and is willing to pay a higher price for, despite it being associated or marketed for a particular gender. The effect of words on price is ultimately approximated through its coefficient in a regularized linear regression on price. There is some convergence amongst the different vectorizors used, ranging from counts, TF-IDF, and one-hot-encoding. Both TF-IDF and one-hot produces a more visible seperation of word importance. Ultimately, on-hot encoding produces the most uniform and easily interpretable features in terms of whether the word is there or not and how its presence can directly increment the price.</p>

    <p>Word count vectorizer: difficult to compare impact one on one as is confounded by occurence. <img src="/static/documents/countvect.png" width="600" height="600"></p>

    <p>TF-IDF vectorizer: coefficients become on different scales<img src="/static/documents/tf-idf.png" width="600" height="600"></p>

    <p>One-hot-encoding: easy to pick out and compare words on a present or not basis<img src="/static/documents/bow.png" width="600" height="600"></p>

    <p>Below are some of the model metrics. R^2 (coefficient of determination) regression score was used as the metric of determining performance. </p>

    <p>Count vectorizer with Ridge Regression. Grid search with 5-fold CV optimized parameters: alpha=125. Training score: 0.6070830419658093, Test Score: 0.25881681222617275</p>

    <p>Bag of Words with Random Forest. Grid search with 5-fold CV optimized parameters: n_estimators=40, min_samples_split=5. Training score:     0.7877491838106718, Test score: 0.24379816561123582</p>


    <p>Count vectorizer with Multi-Layer Perceptron Regressor. Grid search with 5-fold CV optimized parameters: hidden_layer_sizes=(64, 64, 64), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’. Training Score: 0.8473957070129323, Test Score: 0.12217477801362671</p>


    <p>TF-IDF vectorizer with Multi-Layer Perceptron Regressor. Grid search with 5-fold CV optimized parameters: hidden_layer_sizes=(64, 64), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’. Training score: 0.8269466830518402, Test score: 0.008186246482212578</p>


    <p>TF-IDF vectorizer with Random Forest Regressor. Grid search with 5-fold CV optimized parameters: n_estimators=30, min_samples_split=7. Training score: 0.7755749737071362, Test score: 0.2869409630171266</p>

    <p>TF-IDF vectorizer with Lasso Regression did not converge as it required alpha parameter to be small for reasonable regression score.</p>

    <p>TF-IDF vectorizer with Ridge Regression. Grid search with 5-fold CV optimized parameters: alpha=1. Training score: 0.6260793349420096, Test score: 0.26070968786733084.</p>

    <p>Bag of words vectorized model with Ridge regression. Grid search with 5-fold CV optimized parameters: alpha = 41. Train score: 0.6916899250520833 Test score: .25961868867438115. This was ultimately the model chosen as it is most straightforward in interpretation and achieves relative high performance.




</div>
    {% endblock %}
